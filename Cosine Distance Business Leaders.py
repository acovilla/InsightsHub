# -*- coding: utf-8 -*-
"""Business Leaders Leadership Signals NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FfBt1aG3GTeNPF90VyWDtkkwvAVSlRn1
"""

import pandas as pd
import re

tweets_df =  pd.read_csv('business_leaders_tweets.csv')

#Filter out non-original tweets
tweets_df = tweets_df[tweets_df["type_of_tweet"] == "Original"]

# Preprocessing function
def preprocess_tweet(text):
  text = text.lower()
  text = re.sub(r"http\S+|www\S+|https\S+", "", text, flags=re.MULTILINE)
  text = re.sub(r"@[A-Za-z0-9]+", "", text)
  text = re.sub(r"#[A-Za-z0-9]+", "", text)
  text = re.sub(r"[^A-Za-z0-9 ]+", "", text)
  text = re.sub(r"\brt\b", "", text, flags=re.IGNORECASE)
  text = re.sub(r"\bamp\b", "", text, flags=re.IGNORECASE)
  if not text.strip():  # Check if the processed text is empty or contains only whitespace
    return "empty_tweet"
  return text

tweets_df["processed_text"] = tweets_df["text"].apply(preprocess_tweet)

print(tweets_df.shape)

from sentence_transformers import SentenceTransformer

# Load the sentence transformer model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Function to generate embeddings
def get_embedding(text):
  embedding = model.encode(text)
  return embedding

tweets_df["embedding"] = tweets_df["processed_text"].apply(get_embedding)

print(tweets_df.shape)

pip install datasets

from datasets import load_dataset

from sklearn.metrics.pairwise import cosine_similarity

# Load Simple English Wikipedia
general_corpus = load_dataset("wikipedia", "20220301.simple")

#Cosine distance
general_corpus_text = " ".join([article["text"] for article in general_corpus["train"]])

general_corpus_embedding = model.encode(general_corpus_text)

def calculate_cosine_distance(text, embedding):
  if text == "empty_tweet":
        return 0
  words = text.split()
  word_embeddings = model.encode(words)

  distance_from_corpus = 1 - cosine_similarity(embedding.reshape(1, -1), general_corpus_embedding.reshape(1, -1))[0][0]

  return distance_from_corpus

tweets_df["distance_from_wikipedia_corpus"] = tweets_df.apply(
    lambda row: calculate_cosine_distance(row["processed_text"], row["embedding"]), axis=1
)

pip install pingouin

import numpy as np

# Control variable: Log-transform follower count
tweets_df['log_user_followers'] = np.log1p(tweets_df['user_followers_count'])

# Dependent variables: Log-transform engagement metrics (retweets and likes)
tweets_df['log_retweet_count'] = np.log1p(tweets_df['retweet_count'])
tweets_df['log_like_count'] = np.log1p(tweets_df['like_count'])

import pingouin as pg


# Calculate partial correlations between cosine distance and engagement metrics, controlling for follower count
for metric in ['log_retweet_count', 'log_like_count']:  # Using the log-transformed metrics
    result = pg.partial_corr(data=tweets_df,
                             x='distance_from_wikipedia_corpus',
                             y=metric,
                             covar='log_user_followers',
                             method='pearson')  # Spearman is another option
    print(f"Partial correlation with {metric}:\n", result)

pip install seaborn

pip install matplotlib

import seaborn as sns
import matplotlib.pyplot as plt

sns.pairplot(tweets_df[['distance_from_wikipedia_corpus', 'log_retweet_count', 'log_like_count', 'log_user_followers']])
plt.show()

#Check for normality of residuals for like count
model = pg.linear_regression(tweets_df[['distance_from_wikipedia_corpus', 'log_user_followers']], tweets_df['log_like_count'])
sns.histplot(model.residuals_)
plt.show()

import pingouin as pg

# Partial Spearman correlation between cosine distance and log_retweet_count, controlling for log_user_followers
result_spearman_retweets = pg.partial_corr(data=tweets_df,
                                         x='distance_from_wikipedia_corpus',
                                         y='log_retweet_count',
                                         covar='log_user_followers',
                                         method='spearman')
print(f"Partial Spearman correlation with log_retweet_count:\n", result_spearman_retweets)

# Partial Spearman correlation between cosine distance and log_like_count, controlling for log_user_followers
result_spearman_likes = pg.partial_corr(data=tweets_df,
                                      x='distance_from_wikipedia_corpus',
                                      y='log_like_count',
                                      covar='log_user_followers',
                                      method='spearman')
print(f"Partial Spearman correlation with log_like_count:\n", result_spearman_likes)